AI Overview
Here is an example of how to implement the Bag of Words (BoW) model in Python:
1. Data Preparation
First, you need a set of text documents. Let's create a simple example:
Python

documents = [
    "The cat sat on the mat.",
    "The dog chased the cat.",
    "The bird flew away."
]
2. Tokenization
Tokenization is the process of breaking down text into individual words (tokens). We can use the word_tokenize function from the NLTK library: 
Python

import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt') # Download the 'punkt' resource

tokenized_documents = [word_tokenize(doc.lower()) for doc in documents]
3. Vocabulary Creation
Next, create a vocabulary of unique words from all documents:
Python

vocabulary = set()
for doc in tokenized_documents:
    vocabulary.update(doc)
vocabulary = list(vocabulary)
4. Vectorization
Now, convert each document into a vector by counting the occurrences of each word in the vocabulary:
Python

def document_to_vector(doc, vocabulary):
    vector = [0] * len(vocabulary)
    for word in doc:
        if word in vocabulary:
            index = vocabulary.index(word)
            vector[index] += 1
    return vector

bow_vectors = [document_to_vector(doc, vocabulary) for doc in tokenized_documents]
5. Displaying Results
Finally, let's print the vocabulary and the BoW vectors:
Python

print("Vocabulary:", vocabulary)
for i, vector in enumerate(bow_vectors):
    print(f"Document {i+1}: {documents[i]}")
    print(f"Vector: {vector}\n")
Complete Code:
Python

import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt')

documents = [
    "The cat sat on the mat.",
    "The dog chased the cat.",
    "The bird flew away."
]

tokenized_documents = [word_tokenize(doc.lower()) for doc in documents]

vocabulary = set()
for doc in tokenized_documents:
    vocabulary.update(doc)
vocabulary = list(vocabulary)

def document_to_vector(doc, vocabulary):
    vector = [0] * len(vocabulary)
    for word in doc:
        if word in vocabulary:
            index = vocabulary.index(word)
            vector[index] += 1
    return vector

bow_vectors = [document_to_vector(doc, vocabulary) for doc in tokenized_documents]

print("Vocabulary:", vocabulary)
for i, vector in enumerate(bow_vectors):
    print(f"Document {i+1}: {documents[i]}")
    print(f"Vector: {vector}\n")
This code will output the vocabulary and the corresponding vector for each document. Each vector represents the frequency of each word in the vocabulary within that specific document. This is the basic principle of the Bag of Words model.

##구글에 Bow python example치니 AI가 요약해줌
